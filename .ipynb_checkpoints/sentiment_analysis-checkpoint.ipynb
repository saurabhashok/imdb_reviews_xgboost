{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6283fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.72.0\n",
      "  Downloading sagemaker-1.72.0.tar.gz (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.17.79)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.15.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.4.2)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.79 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.20.79)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.79->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.79->boto3>=1.14.12->sagemaker==1.72.0) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-1.72.0-py2.py3-none-any.whl size=386358 sha256=32b96b8dc96b6a5b82333208c59bb7bd20ad0234cf42c89e5563633b5e7ad303\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/58/70/85faf4437568bfaa4c419937569ba1fe54d44c5db42406bbd7\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.42.0\n",
      "    Uninstalling sagemaker-2.42.0:\n",
      "      Successfully uninstalled sagemaker-2.42.0\n",
      "Successfully installed sagemaker-1.72.0 smdebug-rulesconfig-0.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.72.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae0810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-27 16:15:07--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  24.8MB/s    in 3.8s    \n",
      "\n",
      "2021-05-27 16:15:11 (21.0 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20096c18",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a960511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary files \n",
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(dir='../data/aclImdb'):\n",
    "    \"\"\" Segregate data into training and testing data\"\"\"\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(dir, data_type,sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                     \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "             \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b9bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / neg\".format(\n",
    "       len(data['train']['pos']), len(data['train']['neg']),\n",
    "       len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c259c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare data for training and testing from IMDB reviews\"\"\"\n",
    "    \n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7002c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDB reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4383eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quote I used for my summary occurs about halfway through THE GOOD EARTH, as a captain of a Chinese revolutionary army (played by Philip Ahn) apologizes to a mob for not having time to shoot MORE of the looters among them, as his unit has just been called back to the front lines. Of course, the next looter about to be found out and shot is the main character of the film, the former kitchen slave girl O-Lan (for whose portrayal Luise Rainer, now 99-years-old, won her second consecutive best actress Oscar).<br /><br />The next scene finds O-Lan dutifully delivering her bag of looted jewels to her under-appreciative husband, farmer Wang Lung (Paul Muni), setting in motion that classic dichotomy of a man\\'s upward financial mobility being the direct inverse of his moral decline.<br /><br />For a movie dealing with subject matter including slavery, false accusations, misogyny, starvation, home invasion, eating family pets, mental retardation, infanticide, exploited refugees, riots, civil war, summary mass street executions, bigamy, child-beating, adultery, incest, and insect plagues of biblical proportions, THE GOOD EARTH is a surprisingly heart-warming movie.<br /><br />My parting thought is in the form of another classic quote, from O-Lan herself (while putting the precious soup bone her son has just admitted stealing from an old woman back into the cooking pot after husband Wang Lung had angrily tossed it to the dirt floor on the other side of their hut): \"Meat is meat.\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f0821",
   "metadata": {},
   "source": [
    "###  Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bc4d448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ad08982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, 'html.parser').get_text()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \",text.lower())\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stopwords.words('english')] \n",
    "    words = [PorterStemmer().stem(w) for w in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50928d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quot',\n",
       " 'use',\n",
       " 'summari',\n",
       " 'occur',\n",
       " 'halfway',\n",
       " 'good',\n",
       " 'earth',\n",
       " 'captain',\n",
       " 'chines',\n",
       " 'revolutionari',\n",
       " 'armi',\n",
       " 'play',\n",
       " 'philip',\n",
       " 'ahn',\n",
       " 'apolog',\n",
       " 'mob',\n",
       " 'time',\n",
       " 'shoot',\n",
       " 'looter',\n",
       " 'among',\n",
       " 'unit',\n",
       " 'call',\n",
       " 'back',\n",
       " 'front',\n",
       " 'line',\n",
       " 'cours',\n",
       " 'next',\n",
       " 'looter',\n",
       " 'found',\n",
       " 'shot',\n",
       " 'main',\n",
       " 'charact',\n",
       " 'film',\n",
       " 'former',\n",
       " 'kitchen',\n",
       " 'slave',\n",
       " 'girl',\n",
       " 'lan',\n",
       " 'whose',\n",
       " 'portray',\n",
       " 'luis',\n",
       " 'rainer',\n",
       " '99',\n",
       " 'year',\n",
       " 'old',\n",
       " 'second',\n",
       " 'consecut',\n",
       " 'best',\n",
       " 'actress',\n",
       " 'oscar',\n",
       " 'next',\n",
       " 'scene',\n",
       " 'find',\n",
       " 'lan',\n",
       " 'duti',\n",
       " 'deliv',\n",
       " 'bag',\n",
       " 'loot',\n",
       " 'jewel',\n",
       " 'appreci',\n",
       " 'husband',\n",
       " 'farmer',\n",
       " 'wang',\n",
       " 'lung',\n",
       " 'paul',\n",
       " 'muni',\n",
       " 'set',\n",
       " 'motion',\n",
       " 'classic',\n",
       " 'dichotomi',\n",
       " 'man',\n",
       " 'upward',\n",
       " 'financi',\n",
       " 'mobil',\n",
       " 'direct',\n",
       " 'invers',\n",
       " 'moral',\n",
       " 'declin',\n",
       " 'movi',\n",
       " 'deal',\n",
       " 'subject',\n",
       " 'matter',\n",
       " 'includ',\n",
       " 'slaveri',\n",
       " 'fals',\n",
       " 'accus',\n",
       " 'misogyni',\n",
       " 'starvat',\n",
       " 'home',\n",
       " 'invas',\n",
       " 'eat',\n",
       " 'famili',\n",
       " 'pet',\n",
       " 'mental',\n",
       " 'retard',\n",
       " 'infanticid',\n",
       " 'exploit',\n",
       " 'refuge',\n",
       " 'riot',\n",
       " 'civil',\n",
       " 'war',\n",
       " 'summari',\n",
       " 'mass',\n",
       " 'street',\n",
       " 'execut',\n",
       " 'bigami',\n",
       " 'child',\n",
       " 'beat',\n",
       " 'adulteri',\n",
       " 'incest',\n",
       " 'insect',\n",
       " 'plagu',\n",
       " 'biblic',\n",
       " 'proport',\n",
       " 'good',\n",
       " 'earth',\n",
       " 'surprisingli',\n",
       " 'heart',\n",
       " 'warm',\n",
       " 'movi',\n",
       " 'part',\n",
       " 'thought',\n",
       " 'form',\n",
       " 'anoth',\n",
       " 'classic',\n",
       " 'quot',\n",
       " 'lan',\n",
       " 'put',\n",
       " 'preciou',\n",
       " 'soup',\n",
       " 'bone',\n",
       " 'son',\n",
       " 'admit',\n",
       " 'steal',\n",
       " 'old',\n",
       " 'woman',\n",
       " 'back',\n",
       " 'cook',\n",
       " 'pot',\n",
       " 'husband',\n",
       " 'wang',\n",
       " 'lung',\n",
       " 'angrili',\n",
       " 'toss',\n",
       " 'dirt',\n",
       " 'floor',\n",
       " 'side',\n",
       " 'hut',\n",
       " 'meat',\n",
       " 'meat']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train_X[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc587a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")\n",
    "os.makedirs(cache_dir, exist_ok = True)\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available\"\"\"\n",
    "    \n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    if cache_data is None:\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test =  [review_to_words(review) for review in data_test ]\n",
    "        \n",
    "    if cache_file is not None:\n",
    "            cache_data = dict(words_train = words_train , words_test = words_test,\n",
    "                              labels_train = labels_train, labels_test = labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache files:\", cache_file)\n",
    "    else:\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data[\"words_train\"],\n",
    "                cache_data[\"words_test\"], cache_data[\"labels_train\"], cache_data[\"labels_test\"])\n",
    "        \n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfdb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad1df3",
   "metadata": {},
   "source": [
    "### Extract Bag-of-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b469dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def extract_bow_features(words_train, words_test, vocabulary_size=5000, \n",
    "                         cache_dir = cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    if cache_data is None:\n",
    "        vectorizer =  CountVectorizer(max_features= vocabulary_size, \n",
    "                                     preprocessor = lambda x:x, tokenizer = lambda x:x)\n",
    "        \n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "        \n",
    "        features_test = vectorizer.transform(words_test).toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
